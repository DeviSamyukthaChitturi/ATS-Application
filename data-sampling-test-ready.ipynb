{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e69c6751-2999-49ee-8c7d-f2a925307232",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**DATA SAMPLING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b6bd525-4044-4d80-8d63-7a4df81a8d94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Job Description Rows: 5001, Resume Rows: 962\n",
      "Performing Cartesian product...\n",
      "Total records after Cartesian product: 4810962\n",
      "Saving sampled dataset to: /FileStore/tables/sampled_datasets.csv\n",
      "Sampled dataset saved successfully to: /FileStore/tables/sampled_datasets.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def combine_and_sample_correctly_with_multiple_columns(job_desc_file, resumes_file, output_file, sample_size=5700):\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Combine and Sample Datasets with Multiple Columns\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.sql.csv.multiLine\", \"true\") \\\n",
    "        .config(\"spark.sql.csv.escape\", '\"') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    print(\"Loading datasets...\")\n",
    "\n",
    "    # Load the job descriptions dataset\n",
    "    job_desc_df = spark.read.csv(job_desc_file, header=True, inferSchema=True)\n",
    "\n",
    "    # Load the resumes dataset with multiline support\n",
    "    resumes_df = spark.read.option(\"multiLine\", \"true\") \\\n",
    "                           .option(\"quote\", '\"') \\\n",
    "                           .option(\"escape\", '\"') \\\n",
    "                           .option(\"header\", \"true\") \\\n",
    "                           .csv(resumes_file)\n",
    "\n",
    "    # Verify data\n",
    "    if job_desc_df.count() == 0 or resumes_df.count() == 0:\n",
    "        raise ValueError(\"One of the input datasets is empty. Please check the input files.\")\n",
    "\n",
    "    print(f\"Job Description Rows: {job_desc_df.count()}, Resume Rows: {resumes_df.count()}\")\n",
    "\n",
    "    # Perform Cartesian product\n",
    "    print(\"Performing Cartesian product...\")\n",
    "    combined_df = job_desc_df.crossJoin(resumes_df)\n",
    "\n",
    "    # Total number of rows after Cartesian product\n",
    "    total_records = combined_df.count()\n",
    "    print(f\"Total records after Cartesian product: {total_records}\")\n",
    "\n",
    "    # Sample the specified number of records\n",
    "    if sample_size > total_records:\n",
    "        print(f\"Warning: Sample size {sample_size} exceeds total records {total_records}. Taking all records.\")\n",
    "        sampled_df = combined_df\n",
    "    else:\n",
    "        sampled_df = combined_df.sample(withReplacement=False, fraction=sample_size / total_records, seed=1)\n",
    "\n",
    "    print(f\"Saving sampled dataset to: {output_file}\")\n",
    "    # Save the sampled dataset to a CSV file\n",
    "    sampled_df.write.option(\"quote\", '\"') \\\n",
    "                    .option(\"escape\", '\"') \\\n",
    "                    .option(\"multiLine\", \"true\") \\\n",
    "                    .csv(output_file, header=True, mode=\"overwrite\")\n",
    "    print(f\"Sampled dataset saved successfully to: {output_file}\")\n",
    "\n",
    "# File paths\n",
    "job_desc_file = \"/FileStore/tables/jd.csv\"\n",
    "resumes_file = \"/FileStore/tables/UpdatedResumeDataSet.csv\"\n",
    "output_file = \"/FileStore/tables/sampled_datasets.csv\"\n",
    "\n",
    "# Sample size\n",
    "sample_size = 5700\n",
    "\n",
    "combine_and_sample_correctly_with_multiple_columns(job_desc_file, resumes_file, output_file, sample_size)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ATS_EDA",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
